{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with NLP Binary Classifier \n",
    "\n",
    "### Overview\n",
    "This is a machine learning project on Natural Language Processing. The task given is to make binary predictions about the sentiment of the comments given in the stock_data.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('stock_data.csv')\n",
    "text = data['Text'].astype(\"string\")\n",
    "sentiment = data['Sentiment']\n",
    "# convert -1 in sentiment to 0\n",
    "sentiment = pd.DataFrame([0 if int(label) == -1 else 1 for label in sentiment])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0    0\n",
      "dtype: int64\n",
      "1    3685\n",
      "0    2106\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(text.isnull().sum())\n",
    "print(sentiment.isnull().sum())\n",
    "print(sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is generally cleaned, but there is some class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Using CountVectorizer and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation and test sets\n",
    "train_texts, tmp_texts, train_labels, tmp_labels = train_test_split(\n",
    "    text, sentiment, test_size=0.3, random_state=42\n",
    ")\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    tmp_texts, tmp_labels, test_size=0.6, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kicker',\n",
       " 'watchlist',\n",
       " 'xide',\n",
       " 'tit',\n",
       " 'soq',\n",
       " 'pnk',\n",
       " 'cpw',\n",
       " 'bpz',\n",
       " 'aj',\n",
       " 'trade',\n",
       " 'method',\n",
       " 'method',\n",
       " 'see',\n",
       " 'prev',\n",
       " 'post']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # remove urls\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    processed_text = pattern.sub(r'', sentence)\n",
    "    processed_text = re.sub(r'RT[\\s]+', '', processed_text)\n",
    "    processed_text = re.sub(r'@\\S+', '', processed_text)\n",
    "    # remove non-ascii characters\n",
    "    processed_text = processed_text.encode('ascii', 'ignore').decode()\n",
    "    # convert to lowercase\n",
    "    processed_text = nltk.word_tokenize(str(processed_text).lower())\n",
    "    # remove numbers (to be assessed if this is a good idea)\n",
    "    processed_text = [word for word in processed_text if not word.isdigit()]\n",
    "    # remove stopwords and punctuations and the word 'user'\n",
    "    stop_words = set.union(set(stopwords.words('english')), set(string.punctuation), set('user'))\n",
    "    processed_text = [word for word in processed_text if word not in stop_words]\n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def penn2morphy(penntag):\n",
    "        \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "        morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                    'VB':'v', 'RB':'r'}\n",
    "        try:\n",
    "            return morphy_tag[penntag[:2]]\n",
    "        except:\n",
    "            return 'n' \n",
    "    def lemmatize_sent(text):\n",
    "        # Text input is string, returns lowercased strings.\n",
    "        return [lemmatizer.lemmatize(word, pos=penn2morphy(tag)) for word, tag in pos_tag(text)]\n",
    "    processed_text = lemmatize_sent(processed_text) \n",
    "    return processed_text\n",
    "\n",
    "preprocess_sentence(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=preprocess_sentence)\n",
    "train_X = count_vect.fit_transform(train_texts)\n",
    "val_X = count_vect.transform(val_texts)\n",
    "test_X = count_vect.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that appeared once:  4552\n",
      "Number of words that appeared twice:  1063\n",
      "Number of words that appeared 3 times:  471\n",
      "Total number of words:  7668\n"
     ]
    }
   ],
   "source": [
    "train_X_df = pd.DataFrame(train_X.toarray(), columns=count_vect.get_feature_names_out())\n",
    "features_count = train_X_df.sum(axis=0)\n",
    "print(\"Number of words that appeared once: \", len(features_count[features_count == 1]))\n",
    "print(\"Number of words that appeared twice: \", len(features_count[features_count == 2]))\n",
    "print(\"Number of words that appeared 3 times: \", len(features_count[features_count == 3]))\n",
    "print(\"Total number of words: \", len(features_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there are a lot of low frequency words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer=preprocess_sentence, min_df=3)\n",
    "train_X = count_vect.fit_transform(train_texts)\n",
    "val_X = count_vect.transform(val_texts)\n",
    "test_X = count_vect.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit(train_X, train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.60      0.63       252\n",
      "           1       0.78      0.84      0.81       443\n",
      "\n",
      "    accuracy                           0.75       695\n",
      "   macro avg       0.73      0.72      0.72       695\n",
      "weighted avg       0.75      0.75      0.75       695\n",
      "\n",
      "Accuracy on validation set: 0.7511\n",
      "f1 score on validation set: 0.8113\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.64      0.65       367\n",
      "           1       0.81      0.82      0.82       676\n",
      "\n",
      "    accuracy                           0.76      1043\n",
      "   macro avg       0.74      0.73      0.73      1043\n",
      "weighted avg       0.76      0.76      0.76      1043\n",
      "\n",
      "Accuracy on test set: 0.7593\n",
      "f1 score on test set: 0.8161\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# predictions\n",
    "predictions_valid = nb_clf.predict(val_X)\n",
    "print(classification_report(val_labels[0], predictions_valid))\n",
    "\n",
    "print(\"Accuracy on validation set: {:.4f}\".format(accuracy_score(val_labels[0], predictions_valid)))\n",
    "print(\"f1 score on validation set: {:.4f}\".format(f1_score(val_labels[0], predictions_valid)))\n",
    "\n",
    "# test set:\n",
    "predictions_test = nb_clf.predict(test_X)\n",
    "print(classification_report(test_labels[0], predictions_test))\n",
    "\n",
    "print(\"Accuracy on test set: {:.4f}\".format(accuracy_score(test_labels[0], predictions_test)))\n",
    "print(\"f1 score on test set: {:.4f}\".format(f1_score(test_labels[0], predictions_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other variations were considered, for example, keeping the numbers during preprocessing and changing the minimum words frequency to be kept. However, the current model seems to be the best performing in terms of both f1 and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Using Fine-tuned DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1040,  2361,  ...,     0,     0,     0],\n",
      "        [  101, 25430,  2100,  ...,     0,     0,     0],\n",
      "        [  101,  3504,  2066,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 19387,  1030,  ...,     0,     0,     0],\n",
      "        [  101,  3795, 15768,  ...,     0,     0,     0],\n",
      "        [  101,  6522,  4160,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8bd48597c2459ca237bb13feaf1575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCM\\AppData\\Local\\Temp\\ipykernel_40860\\2158006811.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\HCM\\AppData\\Local\\Temp\\ipykernel_40860\\2158006811.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4186, 'learning_rate': 7.874015748031496e-07, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCM\\AppData\\Local\\Temp\\ipykernel_40860\\2158006811.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\HCM\\AppData\\Local\\Temp\\ipykernel_40860\\2158006811.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 14295.2869, 'train_samples_per_second': 0.567, 'train_steps_per_second': 0.036, 'train_loss': 0.4156319001528222, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac26cef7ef0465a806ab61b77601187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44967523217201233, 'eval_f1': 0.851063829787234, 'eval_runtime': 425.9066, 'eval_samples_per_second': 1.632, 'eval_steps_per_second': 0.103, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCM\\AppData\\Local\\Temp\\ipykernel_40860\\2158006811.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\HCM\\AppData\\Local\\Temp\\ipykernel_40860\\2158006811.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772e50220c5a4a96adf0b7ebb68eff3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1218294  -1.1930736 ]\n",
      " [ 0.49513736 -0.4262538 ]\n",
      " [-1.2042898   1.3345178 ]\n",
      " ...\n",
      " [-2.2954617   2.4387577 ]\n",
      " [-0.32987761  0.46210662]\n",
      " [-2.2696893   2.3852952 ]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Tokenize the training and validation sets\n",
    "train_encodings = tokenizer(train_texts.tolist(), add_special_tokens=True, truncation=True, padding='max_length', return_tensors='pt')\n",
    "val_encodings = tokenizer(val_texts.tolist(), add_special_tokens=True, truncation=True, padding='max_length', return_tensors='pt')\n",
    "test_encodings = tokenizer(test_texts.tolist(), add_special_tokens=True, truncation=True, padding='max_length', return_tensors='pt')\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Convert labels to tensor\n",
    "train_labels_tensor = torch.tensor(train_labels.values.flatten(), dtype=torch.long)\n",
    "val_labels_tensor = torch.tensor(val_labels.values.flatten(), dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels.values.flatten(), dtype=torch.long)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels_tensor)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels_tensor)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels_tensor)\n",
    "\n",
    "\n",
    "f1 = evaluate.load('f1')\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return f1.compute(predictions=predictions, references=labels)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=2)\n",
    "\n",
    "# to handle class imbalance\n",
    "weight_for_class_0 = (1 / 3685) \n",
    "weight_for_class_1 = (1 / 2106) \n",
    "class_weights = torch.tensor([weight_for_class_0, weight_for_class_1])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,  # Adjust as needed\n",
    "    per_device_eval_batch_size=16,   # Adjust as needed\n",
    "    weight_decay=0.001,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "print(predictions.predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.73      0.73       367\n",
      "           1       0.86      0.86      0.86       676\n",
      "\n",
      "    accuracy                           0.81      1043\n",
      "   macro avg       0.80      0.80      0.80      1043\n",
      "weighted avg       0.81      0.81      0.81      1043\n",
      "\n",
      "Accuracy on test set: 0.8140\n",
      "f1 score on test set: 0.8567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "predictions = np.argmax(predictions.predictions, axis=1)\n",
    "print(classification_report(test_labels_tensor, predictions))\n",
    "print(\"Accuracy on test set: {:.4f}\".format(accuracy_score(test_labels_tensor, predictions)))\n",
    "print(\"f1 score on test set: {:.4f}\".format(f1_score(test_labels_tensor, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The CountVectorizer and Naive Bayes models model has a f1 score of 0.816 while the DistilBERT model has a gf score of 0.857. DistilBERT seems to be performing slightly better than the Naive Bayes model by about 5%. However, the time taken to train the DistilBERT model was significantly larger.  \n",
    "\n",
    "The trend is smiliar in terms of accuracy, DistilBERT still performed slightly better at 0.814\n",
    "\n",
    "One thing to note is that the training of DistilBERT tries to maximise f1 score instead of accuracy. However, the requirement of the proect was to maximise the accuracy score, so this could possibly be higher for the DistilBERT model if it was done correctly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
